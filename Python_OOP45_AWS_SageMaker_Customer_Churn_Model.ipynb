{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Business problem statement >  2 Prepare dataset > 3 Model Training and evaluation > 4 Automatical model tuning > 5 Deployment > 6 AWS Auto scaling > 7 Relative cost of errors \n",
    "\n",
    "#### Losing customer is costly for any business. Identifying unhappy customers early gives you a chance to offer them incentives to stay. For a telecommunication company, If the company is aware that a particular customer is consdiering leaving, it can offer timely incentives, perhapes in the form of a phone upgrade, or monthly fee discount to encourage the customer ot continue service. Incentives are often more cost effective than losing and reacquiring new customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!head './churn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "churn = pd.read_csv('./churn.txt')\n",
    "churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.describe(included='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create histogram to see how the values of individual attributes are distributed, as well as compute summary statistics for numeric attributes such as mean, min, max and standard deviation ect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show frequency tables for each categorical feature and counts of unique values\n",
    "for column in churn.select_dtypes(included=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column],\n",
    "                       columns = '% observations',\n",
    "                       normalize = 'columns'))\n",
    "    print(\"# of unique values {}\".format(churn[column].nunique()))\n",
    "# show summary statistics\n",
    "display(churn,describe())\n",
    "# build histograms for each numeric feature\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=Ture, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram to check how each feature relates to our target variable churn\n",
    "churn.hist()\n",
    "display(churn.corr(numeric_only='true'))\n",
    "pd.plotting.scatter_matrix(churn,figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seanborn as sn\n",
    "sn.heatmap(churn.corr(numeric_only='true'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several features that essentially have 100 percent correlation with one another. Including these feature pairs in some mahcine learning algorithms can create catastropic problems, while in others it will only introduce minor redundancy and bias. So, need remove the columns that observed as unless for our purpose. Phone and Area code attributes should be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Phone','Area code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, remove one feature from each of the highly correlated pairs: Day Charge: Day Mins, Eve Charge: Eve Mins, Night Charge: Night Mins, Intl Charge: Intl Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge','Eve Charge','Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.head()\n",
    "# Convert the categorical features into numeric features\n",
    "model_data = pd.get_dummies(churn, dtype='int')\n",
    "model_data = pd.concat([Model_data['Churn?_True.'], model_data.drop(['Churn?_False'.,'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation and test sets. This will help prevent overfitting the model\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7* len(model_data),\n",
    "     int(0.9*len(model_data))]) \n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('Validation.csv',header=False, index=False)                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to Amazon S3\n",
    "import sys\n",
    "! {sys.excutable} -m pip install sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'bootcamp-xgboost-churn'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train_csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "#### Amazon Sagemaker algorithms are packaged as Docker images. This provides the flexibility to use almost any algorithm code with Amazon Sagemaker regardless of implementation language, dependent libraries, frameworks, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set IAM Role\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "# Get the XGBoost docker image\n",
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve('xgboost',boto3.Session().region_name, '1.0-1')\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Python SDK provides high-level abstractions for working with Amazon SageMaker: \n",
    "  * Estimators: Encapsulates training on SageMaker\n",
    "  * Models: Encapsulates built Ml models\n",
    "  * Predictors: Provides real-time inference and transformation using Python data-types against a SageMaker endpoint\n",
    "  * Session: Prodvides a collection of methods for working with SegaMaker resources\n",
    "#### Start by creating the xgboost_Estimator, the mandatory parameters are: image_url, role, session, instance_type, and instance_count. For this training job, we use below parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat the SageMaker Estimator object\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(container,role, instance_count=1, instance_type='ml.m4.xlarge'\n",
    "                                   ,output_path = 's3://{}/{}/output'.format(bucket,prefix),\n",
    "                                   sagemaker_session=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
